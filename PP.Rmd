---
title: "Prix de vente de maisons à King County"
author: "Matis Brassard-Verrier, Alyson Marquis, Alexis Picard et Samuel Provencher"
output:
     powerpoint_presentation:
        reference_doc : Presentation1.pptx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
source("script.R")
source("1-ModeleBase.R")
source("2-Lasso.R")
source("3-K_voisin.R")
source("4-Arbre.R")
load("save.Rdata")
load("save2.Rdata")
load("save3.Rdata")
```

## Introduction


## Plan de la présentation

- Description du problème
- Base de données à l'étude
- Prétraitement des données
- Analyse exploratoire
- Modèles testés
- Comparaison des modèles
- Forêt aléatoire
- GBM
- Conclusion

## Description du problème

- Quelques utilités actuariels
  + Futures soumissions d'assurance habitation
  + Prêts hypothécaire à des fins de hedging

## Base de données à l'étude

- Prix de vente des maisons dans le comté de King County (Washington, USA)
- Maisons vendues entre mai 2014 et mai 2015
- 21 613 lignes pour 21 colonnes (variables)


## Prétraitement des données

- Traitements des erreurs
  + Doublons
  + Suppression de données
- Variables retirées
- Création de variables
  + *age*
  + *reno*
  + *expensive_area*


## Analyse exploratoire - Variable réponse

```{r}
library(gridExtra)

#https://stackoverflow.com/questions/30026887/plot-title-at-bottom-of-plot-using-ggplot2 pour mettre en-dessous

pri <- ggplot(donnees, aes(x=price/1000000)) + geom_density() + ##Pour que le pas de graduation soit beau
  ggtitle("Distribution du prix de vente des maisons") +
  xlab("Prix de vente des maisons (M$)") + ylab("Densité") +
  theme_bw() + #asymétrie
  theme(plot.title = element_text(hjust = 0.5, size = 7), axis.title = element_text(size=6))

Lpri <- ggplot(donnees, aes(x=log(price))) + geom_density() + 
  ggtitle("Distribution du logarithme du prix de vente des maisons") +
  xlab("Logarithme du prix de vente des maisons") + ylab("Densité") +
  theme_bw() + #log pour variable réponse est mieux
  theme(plot.title = element_text(hjust = 0.5, size = 7), axis.title = element_text(size=6))

grid.arrange(pri, Lpri ,nrow = 1, ncol=2)
```

## Analyse exploratoire - Carte thermique

```{r warning=FALSE, message=FALSE}

ggmap(map, extent = "device") + stat_summary_2d(data = donnees ,
                                                aes(x = long, y = lat, z = log(price)),
                                                fun = mean, alpha = 0.6, bins = 100) + 
    scale_fill_gradient(name = "Log(Price)", low = "green", high = "red") +
    annotate("rect", xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, colour="black", lty = 1, lwd = 1.3, alpha = 0) +
    annotate("text", x= -122.2, y = 47.6, label = "Zone urbaine")

```

## Analyse en composantes principales

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Visualisation contributions des 4 composantes
contrib_corrige <- data.frame(acp_corrige$var$coord[,1:4])
contrib_corrige$carac <- rownames(contrib_corrige)
contrib.long_corrige <- reshape2::melt(contrib_corrige)

ggplot(contrib.long_corrige, aes(x=carac, fill=variable, y=value))+
    geom_bar(stat="identity",position=PositionDodge)+
    facet_grid(~variable)+
    theme(legend.position="none",axis.text.x = element_text(angle = 90))+
    coord_flip()+labs(title = "Contributions des variables aux 4 premières composantes") +
  theme(plot.title = element_text(size = rel(0.8))) +
  ylab("Contribution") +
  xlab("Variable")

```


## Modèles testés

- Modèle de base
- Modèle linéaire généralisé avec régression Lasso
- Modèle des *k* plus proches voisins
- Arbre de décision
- Ensemble d'arbres de décision agrégées par *bagging*
- Forêt aléatoire
- Modèle de *gradient boosting*

## Modèles testés - Modèle de base

- Régression linéaire multiple avec transformation logarithmique
- Aucune interaction
- Aucune sélection formelle de variables

## Modèles testés - Modèle linéaire généralisé avec régression Lasso

## Modèles testés - Modèle des *k* plus proches voisins

## Modèles testés - Arbre de décision

- Optimisation de `minbucket`
- Optimisation du paramètre de compléxité (`cp`)
- `method="anova"`, donc la fonction de perte est l'EQM
- Arbre élagué trop gros pour être représenté graphiquement

## Modèles testés - Ensemble d'arbres de décision agrégées par *bagging*

- `sampsize= nrow(donnees.train)`
- `mtry=17`
- `cp=0`
- `nodesize=5`
- `ntree=500`

```{r echo=F,eval=T, message=FALSE}
plot(1:length(bag$mse),bag$mse, type="l", xlab="Nombre d'arbres composant le bagging", ylab="EQM des OOB", main="Graphique X: EQM des observations OOB \nen fonction du nombre d'arbres")
```

## Modèle testés - Forêt aléatoire

- `sampsize= 0.75*nrow(donnees.train)`
- `cp=0`
- `nodesize=5`
- `ntree=150`
- `mtry=8`

```{r echo=F,eval=T, message=FALSE}
plot(rf.train, main="Graphique X: EQM des observations OOB \nen fonction du mtry", xlab = "Nombre de prédicteurs choisis aléatoirement (m)", ylab = "L'erreur de validation croisée")
```

## Modèles testés - Modèle de *gradient boosting*

## Comparaison des modèles

```{r echo=FALSE}
library(knitr)

dataframe <- data.frame(c("Modèle de base", "Modèle Lasso", "K plus proches voisins","Arbre de décision","Bagging","Forêt aléatoire", "Gradient boosting (GBM)"), c(EQM.ModBase,msep.lasso, EQM.Kvoisins, EQM.arbre, EQM.bag,EQM.foret,3))

kable(dataframe, caption = "L'EQM des sept modèles testés", col.names = c("Modèles","EQM"), digits = 5)
```

## Forêt aléatoire

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(randomForest)
varImpPlot(foret, main="Importance des variables composant \nla forêt aléatoire")
```

## Forêt aléatoire (suite)

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(iml)
plot(pdp.lat) + ggtitle("Graphique de dépendance partielle de la latitude") + xlab("Latitude") + scale_y_continuous(name="Prédiction du logarithme du prix") + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
plot(pdp.long) + ggtitle("Graphique de dépendance partielle de la longitude") + xlab("Longitude") + scale_y_continuous(name="Prédiction du logarithme du prix") + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
plot(pdp.grade) + ggtitle("Graphique de dépendance partielle de la variable grade") + xlab("Grade") + scale_y_continuous(name="Prédiction du logarithme du prix") + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
```

## Forêt aléatoire (suite)
```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(int.lat) + ggtitle("Graphique des statistiques H avec la latitude")+ theme_classic() + theme(plot.title = element_text(hjust = 0.5))
```

## Forêt aléatoire (suite)
```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(pdp.lat.long) + ggtitle("Graphique de dépendance partielle bivarié entre \nla latitude et la longitude")+ theme_classic() + theme(plot.title = element_text(hjust = 0.35))
```

## Modèle de *gradient boosting*

## Conclusion

- Modèle GBM retenu
- Limitations
  + Échelle logarithmique
  + Effet de l'inflation
- Possibilité d'utilisation dans d'autres zones géographiques


## Bibliographie

1. Kaggle, harlfoxen (2017). House sales in King County, USA. Récupéré le 27 février 2020 de https://www.kaggle.com/harlfoxem/housesalesprediction.
\newline
2. Max Kuhn (2020). caret: Classification and Regression Training. R package version 6.0-85. https://CRAN.R-project.org/package=caret
\newline
3. Terry Therneau and Beth Atkinson (2019). rpart: Recursive Partitioning and Regression Trees. R package version 4.1-15. https://CRAN.R-project.org/package=rpart
\newline
4. Stephen Milborrow (2019). rpart.plot: Plot 'rpart' Models: An Enhanced Version of 'plot.rpart'. R package version 3.0.8. https://CRAN.R-project.org/package=rpart.plot

## Bibliographie

5. A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18--22.
\newline
6. Brandon Greenwell, Bradley Boehmke, Jay Cunningham and GBM Developers (2019). gbm: Generalized Boosted Regression Models. R package version 2.1.5. https://CRAN.R-project.org/package=gbm
\newline
7. Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL http://www.jstatsoft.org/v33/i01/.
\newline
8. Alina Beygelzimer, Sham Kakadet, John Langford, Sunil Arya, David Mount and Shengqiao Li (2019). FNN: Fast Nearest Neighbor Search Algorithms and Applications. R package
version 1.1.3.  https://CRAN.R-project.org/package=FNN



